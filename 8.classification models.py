# -*- coding: utf-8 -*-
"""4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iphql-Q8kC7FYOCc6jiwjM_s-td4zm_e
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/ML Concordia/Major Assignment 2/archive/survey lung cancer.csv")

dataframe_num = dataframe
replacement_map = {'M': 1, 'F': 0}
dataframe_num['GENDER'] = dataframe_num['GENDER'].replace(replacement_map)
dataframe_num = dataframe
replacement_map = {'YES': 1, 'NO': 0}
dataframe_num['LUNG_CANCER'] = dataframe_num['LUNG_CANCER'].replace(replacement_map)

dataframe_x = dataframe_num[dataframe_num.columns.difference(['LUNG_CANCER'])]
X_train, X_test, y_train, y_test = train_test_split(dataframe_x, dataframe_num['LUNG_CANCER'], test_size=0.3)

#K-nearest
from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=10)
knn_model.fit(X_train, y_train)

train_predictions = knn_model.predict(X_train)
test_predictions = knn_model.predict(X_test)

print("Classification Report for Training Set:")
print(classification_report(y_train, train_predictions))

print("\nClassification Report for Testing Set:")
print(classification_report(y_test, test_predictions))

#SVM
from sklearn.svm import SVC

svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

train_predictions = svm_model.predict(X_train)
test_predictions = svm_model.predict(X_test)

print("Classification Report for Training Set:")
print(classification_report(y_train, train_predictions))

print("\nClassification Report for Testing Set:")
print(classification_report(y_test, test_predictions))

# Gaussian Naive Bayes
from sklearn.naive_bayes import GaussianNB

gnb_model = GaussianNB()
gnb_model.fit(X_train, y_train)

train_predictions = gnb_model.predict(X_train)
test_predictions = gnb_model.predict(X_test)

print("Classification Report for Training Set:")
print(classification_report(y_train, train_predictions))

print("\nClassification Report for Testing Set:")
print(classification_report(y_test, test_predictions))

#Decision Tree
from sklearn.tree import DecisionTreeClassifier

tree_model = DecisionTreeClassifier()
tree_model.fit(X_train, y_train)

train_predictions = tree_model.predict(X_train)
test_predictions = tree_model.predict(X_test)

print("Classification Report for Training Set:")
print(classification_report(y_train, train_predictions))

print("\nClassification Report for Testing Set:")
print(classification_report(y_test, test_predictions))

#AdaBoost
from sklearn.ensemble import AdaBoostClassifier

adaboost_model = AdaBoostClassifier()
adaboost_model.fit(X_train, y_train)

train_predictions = adaboost_model.predict(X_train)
test_predictions = adaboost_model.predict(X_test)

print("Classification Report for Training Set:")
print(classification_report(y_train, train_predictions))

print("\nClassification Report for Testing Set:")
print(classification_report(y_test, test_predictions))

print('''By default, the AdaBoostClassifier utilizes a decision tree with a maximum depth of 1 (max_depth=1), commonly referred to as a decision stump, as its base estimator.
This decision stump represents a simple tree model comprising a single decision node and two leaf nodes''')

"""#Discuss and compare the performance of the trained models.
The K-nearest model with K=10 has the worst performance, especially in predicting no-cancer dataset. As the report shows, the accuracy of the model is around 50% while the accuracy for predicting cancer is over 90%. This bad forecast does not have much effect on the final accuracy because the number of healthy items is very small. As a result, the report shows an accuracy of prediction around 80-90%.

The SVM and GaussianNB are similar in the accuracy of predictions. They have a high accuracy in training and testing sets. To explain more, SVM has a high accuracy in predicting cancer and acceptable accuracy in predicting no-cancer.  GaussianNB has a similarly high accuracy in the prediction of cancer. While the prediction of no-cancer is less than the same situation in SVM. Finally, the accuracy of both of them is around 90%.

Overfitting occurred in the decision tree. The decision tree correctly predicts all data sets in training, while the prediction accuracy is lower in the testing set. The precision of prediction in the testing set is around 60% in no-cancer and more than 90% in cancer situations. Finally, the accuracy of the testing set is 85%.

AdaBoost predicts cancer and no-cancer of training datasets with a precision of 80-90%, in both situations. The accuracy of prediction of the training dataset is 96%. While the precision for testing dataset is a little different. The precision of predicting cancer and no cancer are around 87% and 62%, respectively. Finally, the accuracy of prediction of testing data is 85%.
"""